Metadata-Version: 2.4
Name: code-explanation-agent
Version: 0.1.0
Summary: AI multi-agent code reviewer using OpenAI, LangGraph, and FastAPI
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.121.3
Requires-Dist: langgraph>=0.2.30
Requires-Dist: langchain>=0.3.27
Requires-Dist: langchain-core>=0.3.80
Requires-Dist: langchain-openai>=0.2.14
Requires-Dist: pydantic>=2.12.4
Requires-Dist: python-dotenv>=1.2.1
Requires-Dist: uvicorn[standard]>=0.38.0
Requires-Dist: radon>=6.0.1
Requires-Dist: bandit>=1.7.9
Requires-Dist: celery>=5.4.0
Requires-Dist: redis>=5.0.8

Backend service with FastAPI + LangGraph. Supports SSE streaming to the Next.js frontend and optional Celery + Redis offloading for high throughput.

Quick start
- Create `.env` from `backend/.env.example` and fill keys.
- Run Redis locally: `docker run -p 6379:6379 redis`.
- Start API: `uvicorn backend.main:app --reload`.
- Optional: enable Celery by setting `USE_CELERY=1` in `.env` and run a worker:
  - `celery -A backend.app.celery_app.celery_app worker -l info -Q celery -c 2`

Notes
- When `USE_CELERY=1`, requests are streamed via Redis pub/sub from workers.
- Semantic cache uses Redis automatically when reachable; otherwise in-memory.
- LangGraph checkpointer can be enabled by `LANGGRAPH_CHECKPOINTER=1`.
